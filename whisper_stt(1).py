# -*- coding: utf-8 -*-
"""Whisper_STT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qd17inrJ_Af9FrrAvFfkJqOwRNgLb6T6

# Installing Whisper

The commands below will install the Python packages needed to use Whisper models and evaluate the transcription results.
"""

! pip install git+https://github.com/openai/whisper.git
! pip install jiwer

import whisper
import jiwer

model=whisper.load_model("medium")
#tokenizer = whisper.load_tokenizer("whisper-tokenizer")

from google.colab import drive
drive.mount('/content/drive')

#result=model.transcribe("/content/drive/MyDrive/Quran_Ayat_public/audio_data/Abdul_Basit_Murattal_64kbps/001002.wav",fp16=False)

# for i in range(1, 8):
#     file_number = f"{i:03d}"  # Format the number with leading zeros
#     file_path = f"/content/drive/MyDrive/Quran_Ayat_public/audio_data/Abdul_Basit_Murattal_64kbps/001{file_number}.wav"
#     result = model.transcribe(file_path, fp16=False)
#     print(result["text"])

#result["text"]

# Define a function to tokenize the text using the Whisper tokenizer
def tokenize_text(text):
    tokens = tokenizer.tokenize(text)
    return tokens

# Read the text file and tokenize each line
with open(text_file_path, "r", encoding="utf-8") as text_file:
    transcripts = [tokenize_text(line.strip()) for line in text_file.readlines()]



folder_names = ["Abdul_Basit_Murattal_64kbps", "Abdullah_Basfar_32kbps", "Abdullah_Basfar_64kbps",
                "AbdulSamad_64kbps", "Abdurrahmaan_As-Sudais_64kbps", "Abu_Bakr_Ash-Shaatree_64kbps", "Alafasy_64kbps", "Ali_Jaber_64kbps",
                "Ayman_Sowaid_64kbps", "Banna_32kbps", "Fares_Abbad_64kbps", "Ghamadi_40kbps", "Hani_Rifai_192kbps", "Hudhaify_64kbps",
                "Husary_64kbps", "Husary_Mujawwad_64kbps", "Hussary.teacher_64kbps", "Ibrahim_Akhdar_32kbps", "Maher_AlMuaiqly_64kbps"]

output_file_path = "/content/drive/MyDrive/Fatiaha_transcript_2.txt"

with open(output_file_path, "w") as output_file:
    for folder_name in folder_names:
        output_file.write(f"Folder: {folder_name}\n")

        print(f"File: {folder_name}")
        output_file.write(f"File: {folder_name}\n")

        for i in range(1, 8):
            file_number = f"{i:03d}"
            file_path = f"/content/drive/MyDrive/Quran_Ayat_public/audio_data/{folder_name}/001{file_number}.wav"

            result = model.transcribe(file_path, fp16=False)

            print(f"Transcript: {result['text']}\n")

            output_file.write(f"Transcript: {result['text']}\n\n")

# Assuming your data is in the 'result' dictionary
result = {
    "Transcript": [
        "بسم الله الرحمن الرحيم",
        "الحمد لله رب العالمين",
        "الرحمن الرحيم",
        "مالك يوم الدين",
        "إياك نعبد وإياك نستعين",
        "إهدنا الصراط المستقيم",
        "صراط الذين أنعمت عليهم غير المغضوب عليهم ولا الضالين"
    ]
}

def extract_unique_words(batch):
    all_text = " ".join(batch["Transcript"])
    words = list(set(all_text.split()))  # Convert to set to remove duplicates
    return {"vocab": [words], "all_text": [all_text]}

# Applying the function to the dictionary
vocab_result_unique_words = extract_unique_words(result)

# Accessing the results
vocab = vocab_result_unique_words["vocab"]
all_text = vocab_result_unique_words["all_text"]

# Print or use the extracted vocabulary and all text as needed
print("Vocabulary:", vocab)
print("All Text:", all_text)

import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')
# Assuming you have the 'all_text' variable from the 'extract_unique_words' function
all_text = "بسم الله الرحمن الرحيم الحمد لله رب العالمين الرحمن الرحيم مالك يوم الدين إياك نعبد وإياك نستعين إهدنا الصراط المستقيم صراط الذين أنعمت عليهم غير المغضوب عليهم ولا الضالين"

# Tokenize the words using nltk
words = word_tokenize(all_text, language='arabic')

# Print or use the tokenized words as needed
print("Tokenized Words:", words)

# Read the text file containing correct transcripts
text_file_path = "/content/drive/MyDrive/Fatihah_all_verse.txt"
with open(text_file_path, "r", encoding="utf-8") as file:
    lines = file.readlines()

# Create a DataFrame to store the results
results_df = pd.DataFrame(columns=["Expected", "Actual", "WER"])

# Assuming you have a list of file paths corresponding to your audio files
audio_file_paths = [
    "/content/drive/MyDrive/Quran_Ayat_public/audio_data/Abdul_Basit_Murattal_64kbps/001001.wav",
    "/content/drive/MyDrive/Quran_Ayat_public/audio_data/Abdul_Basit_Murattal_64kbps/001002.wav",
    "/content/drive/MyDrive/Quran_Ayat_public/audio_data/Abdul_Basit_Murattal_64kbps/001003.wav",
    "/content/drive/MyDrive/Quran_Ayat_public/audio_data/Abdul_Basit_Murattal_64kbps/001004.wav",
    "/content/drive/MyDrive/Quran_Ayat_public/audio_data/Abdul_Basit_Murattal_64kbps/001005.wav",
    "/content/drive/MyDrive/Quran_Ayat_public/audio_data/Abdul_Basit_Murattal_64kbps/001006.wav",
    "/content/drive/MyDrive/Quran_Ayat_public/audio_data/Abdul_Basit_Murattal_64kbps/001007.wav"
    # Add more file paths as needed
]

# Loop through each audio file and its corresponding transcript
for i, (audio_path, expected_transcript) in enumerate(zip(audio_file_paths, lines), start=1):
    # Transcribe the audio using the Whisper model
    result = model.transcribe(audio_path, fp16=False)

    # Extract the actual transcript from the model result
    actual_transcript = result["text"]

    # Calculate Word Error Rate (WER)
    wer = jiwer.wer(expected_transcript, actual_transcript)

    # Print and store the results
    print(f"Audio {i}:")
    print(f"Expected Transcript: {expected_transcript.strip()}")
    print(f"Actual Transcript: {actual_transcript.strip()}")
    print(f"WER: {wer}\n")

    results_df = results_df.append({
        "Expected": expected_transcript.strip(),
        "Actual": actual_transcript.strip(),
        "WER": wer
    }, ignore_index=True)

# Display the overall WER for all audio files
overall_wer = results_df["WER"].mean()
print(f"Overall Word Error Rate: {overall_wer}")



import pandas as pd

f = open("/content/drive/MyDrive/Fatihah_first_verse.txt", "a",encoding="utf-8")
f.write(result["text"])
f.close()