# -*- coding: utf-8 -*-
"""Whisper_Tarteel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xt1UqbFhbrKAq1FF_Q2eeEb701cpMnIn

## Parquet to XLSX
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from sklearn.model_selection import train_test_split

# read wbw data
wbw_file_1 = pd.read_parquet("/content/drive/MyDrive/wbw_1.parquet")
wbw_file_2 = pd.read_parquet("/content/drive/MyDrive/wbw_113.parquet")
wbw_file_3 = pd.read_parquet("/content/drive/MyDrive/wbw_114.parquet")


# Rename the columns in additional_train_df to match the desired column names
wbw_file_1.rename(columns={'wav_filename': 'Path', 'word': 'Transcript'}, inplace=True)
wbw_file_2.rename(columns={'wav_filename': 'Path', 'word': 'Transcript'}, inplace=True)
wbw_file_3.rename(columns={'wav_filename': 'Path', 'word': 'Transcript'}, inplace=True)

# Concatenate the train_wbw and test sets from both DataFrames
wbw_file = pd.concat([wbw_file_1, wbw_file_2, wbw_file_3], ignore_index=True)


# Split the DataFrame into train and test sets
train_wbw, test_wbw = train_test_split(wbw_file, test_size=0.2, random_state=42)

# Load data from a Parquet file into a pandas DataFrame
train_ayah_fatiha = pd.read_parquet("/content/drive/MyDrive/fatiha_train (1).parquet")
test_ayah_fatiha = pd.read_parquet("/content/drive/MyDrive/fatiha_test_(1).parquet")
train_ayah_falaq_nas = pd.read_parquet("/content/drive/MyDrive/train_data.parquet")
test_ayah_falaq_nas = pd.read_parquet("/content/drive/MyDrive/test_data.parquet")

print(train_ayah_fatiha.columns)

# Concatenate the train sets from both DataFrames
train = pd.concat([train_ayah_falaq_nas[['Path', 'Transcript']],train_ayah_fatiha[['Path', 'Transcript']], train_wbw], ignore_index=True)
test = pd.concat([test_ayah_falaq_nas[['Path', 'Transcript']],test_ayah_fatiha[['Path', 'Transcript']], test_wbw], ignore_index=True)


#Export as Excel sheet

# Write train DataFrame to XLSX
train.to_excel('/content/drive/MyDrive/train_1_113_114.xlsx', index=False)

# Write test DataFrame to XLSX
test.to_excel('/content/drive/MyDrive/test_1_113_114.xlsx', index=False)

#make one excel file with two sheets

# Assuming train and test are your DataFrames

# Define the file paths for the Excel files
train_file = '/content/drive/MyDrive/train_1_113_114.xlsx'
test_file = '/content/drive/MyDrive/test_1_113_114.xlsx'

# Create an ExcelWriter object to write to a single file
with pd.ExcelWriter('/content/drive/MyDrive/train_test_1_113_114.xlsx') as writer:
    # Write train DataFrame to the first sheet
    train.to_excel(writer, sheet_name='Train', index=False)

    # Write test DataFrame to the second sheet
    test.to_excel(writer, sheet_name='Test', index=False)

"""## Main Pipeline"""

from google.colab import drive
drive.mount('/content/drive')

#!pip install gdown
#!pip install gradio
#!pip install datasets>=2.6.1
!pip install jiwer
!pip install librosa
!pip install evaluate>=0.30
!pip install accelerate -U
!pip install git+https://github.com/huggingface/transformers

import os
import torch
import librosa
import evaluate
import pandas as pd
from jiwer import wer
from datasets import Audio
from dataclasses import dataclass
from transformers import Seq2SeqTrainer
from transformers import WhisperTokenizer
from transformers import WhisperProcessor
from    typing    import Any, Dict, List, Union
from transformers import WhisperFeatureExtractor
from transformers import Seq2SeqTrainingArguments
from transformers import WhisperForConditionalGeneration
from   datasets   import load_dataset, DatasetDict, Dataset
from transformers import WhisperForConditionalGeneration, WhisperProcessor

def prepare_dataset(batch):
    audio = batch["audio"]

    batch["input_features"] = feature_extractor(audio["array"], sampling_rate=audio["sampling_rate"]).input_features[0]

    batch["labels"] = tokenizer(batch["sentence"]).input_ids
    return batch


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:

        input_features = [{"input_features": feature["input_features"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        label_features = [{"input_ids": feature["labels"]} for feature in features]

        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)


        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch

def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    label_ids[label_ids == -100] = tokenizer.pad_token_id

    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    wer = 100 * metric.compute(predictions=pred_str, references=label_str)

    return {"wer": wer}

def dataset_convert(excel,sheet):
  data = pd.read_excel(excel,sheet_name=sheet)
  audio_files = data["Path"].tolist()
  texts = data["Transcript"].tolist()

  my_dataset = {
      "audio": audio_files,
      "sentence": texts
  }

  my_dataset = Dataset.from_dict(my_dataset)
  return my_dataset

# define function to retrain the model with new data
def retrain_ck_a(data_path, pretrained_m):

  common_voice = DatasetDict()
  common_voice["train"] = dataset_convert(data_path,"Train")#.select(range(5))
  common_voice["test"] = dataset_convert(data_path,"Test")#.select(range(5))

  processor = WhisperProcessor.from_pretrained("openai/whisper-small", language="arabic", task="transcribe")

  common_voice = common_voice.cast_column("audio", Audio(sampling_rate=16000))

  common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names["train"], num_proc=4)

  data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

  model = WhisperForConditionalGeneration.from_pretrained(pretrained_m)

  model.config.forced_decoder_ids = None

  model.config.suppress_tokens = []
  LANGUAGE = "Arabic"
  TASK = "transcribe"

  model.config.forced_decoder_ids = processor.tokenizer.get_decoder_prompt_ids(
  language=LANGUAGE, task=TASK
  )
  model.config.suppress_tokens = []
  model.generation_config.forced_decoder_ids = processor.tokenizer.get_decoder_prompt_ids(
  language=LANGUAGE, task=TASK
  )
  model.generation_config.suppress_tokens = []
  # define training arguments

  training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/whisper_small_processor",
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,
    learning_rate=5e-5,  # increase learning rate for faster convergence on smaller dataset
    warmup_steps=120,
    max_steps=300,
    gradient_checkpointing=True,
    fp16=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=8,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=66,
    eval_steps=66,
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=False,

)

  processor.save_pretrained(training_args.output_dir)

  # initialize trainer object with training arguments and model
  trainer = Seq2SeqTrainer(
      args=training_args,
      model=model,
      train_dataset=common_voice["train"],
      eval_dataset=common_voice["test"],
      data_collator=data_collator,
      compute_metrics=compute_metrics,
      tokenizer=processor.feature_extractor)

  return trainer

metric = evaluate.load("wer")

model =WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")

feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")

tokenizer = WhisperTokenizer.from_pretrained("openai/whisper-small", language="arabic", task="transcribe")

processor = WhisperProcessor.from_pretrained("openai/whisper-small", language="arabic", task="transcribe")

data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)

data="/content/drive/MyDrive/train_test_1_113_114.xlsx"
pretraind_m="openai/whisper-small"

trainer=retrain_ck_a(data,pretraind_m)
trainer.train()


# Define the directory path where you want to save the trained model
output_dir = "/content/drive/MyDrive/1_113_114_trained_model"

# Save the trained model
trainer.save_model(output_dir)

os.listdir("/content/drive/MyDrive/1_113_114_trained_model")

"""## Predictions"""

import IPython.display as ipd
import numpy as np
import random

test_dir = '/content/drive/MyDrive/test_samples/me_01001.wav'

ipd.Audio(data=test_dir, autoplay=True, rate=16000)

"""## **Gemini**"""

# # Import libraries
# import whisper
# from your_model import predict  # Replace with your prediction function

# # Load your trained model (if applicable)

# # Define the new audio file path
# new_audio_path = "/content/drive/MyDrive/test_samples/me_01001.wav"

# # Preprocess the audio (normalization, noise reduction, etc.)

# # Transcribe the audio using Whisper
# model = whisper.load_model("base")  # Replace with your Whisper model choice
# result = model.transcribe(new_audio_path)
# text = result["text"]

# # Preprocess the transcribed text

# # Make a prediction
# prediction = predict(text)

# # Print or interpret the prediction result
# print(f"Predicted output: {prediction}")

def test_model(new_audio_path, model):
  """
  Tests the trained Whisper model on a new audio file.

  Args:
      new_audio_path (str): Path to the new audio file.
      model (WhisperForConditionalGeneration): The trained model for prediction.

  Returns:
      str: The predicted output from the model.
  """

  # Preprocess the audio (replace with your specific steps)
  # Example (assuming normalization between 0 and 1):
  audio, sample_rate = librosa.load(new_audio_path)
  audio = librosa.effects.normalize(audio)

  # Transcribe the audio using Whisper (already in your code)
  processor = WhisperProcessor.from_pretrained("openai/whisper-small", language="arabic", task="transcribe")
  # Load the trained model
  model = WhisperForConditionalGeneration.from_pretrained(output_dir)
  inputs = processor(audio, return_tensors="pt", sampling_rate=sample_rate)
  with torch.no_grad():
      result = model(**inputs)
  text = result["text"]

  # Preprocess the transcribed text (replace with your specific steps)
  # Example (basic cleaning)
  text = text.lower()  # Convert to lowercase
  text = re.sub(r"[^\w\s]", "", text)  # Remove punctuation

  # Make a prediction
  prediction = model.generate(text)  # Assuming your model generates predictions
  return prediction[0]["generated_text"]  # Assuming the output is a list of dictionaries

# Example usage
new_audio_path = "/content/drive/MyDrive/test_samples/me_01001.wav"
predicted_output = test_model(new_audio_path, model)
print(f"Predicted output: {predicted_output}")